/**
 * API Performance Monitor
 *
 * CLAUDE.md ì¤€ìˆ˜: API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
 * ì‘ë‹µ ì‹œê°„, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
 */

import logger from '../logger'
import { apiCache } from './cache-manager'

// ===========================================
// íƒ€ì… ì •ì˜
// ===========================================

interface ApiMetrics {
  endpoint: string
  method: string
  responseTime: number
  statusCode: number
  userId?: string
  userAgent?: string
  ip?: string
  timestamp: number
  error?: string
}

interface PerformanceStats {
  totalRequests: number
  averageResponseTime: number
  medianResponseTime: number
  p95ResponseTime: number
  p99ResponseTime: number
  errorRate: number
  requestsPerMinute: number
  slowestEndpoints: Array<{
    endpoint: string
    averageResponseTime: number
    requestCount: number
  }>
  errorsByEndpoint: Record<string, number>
  responseTimePercentiles: number[]
}

interface EndpointStats {
  requestCount: number
  totalResponseTime: number
  responseTimes: number[]
  errorCount: number
  lastRequestTime: number
}

// ===========================================
// ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
// ===========================================

export class ApiPerformanceMonitor {
  private metrics: ApiMetrics[] = []
  private endpointStats: Map<string, EndpointStats> = new Map()
  private readonly maxMetricsSize = 10000 // ìµœëŒ€ ë©”íŠ¸ë¦­ ë³´ê´€ ê°œìˆ˜
  private readonly performanceThresholds = {
    slow: 1000, // 1ì´ˆ ì´ìƒì´ë©´ ëŠë¦¼
    verySlow: 3000, // 3ì´ˆ ì´ìƒì´ë©´ ë§¤ìš° ëŠë¦¼
    critical: 5000, // 5ì´ˆ ì´ìƒì´ë©´ ì‹¬ê°
  }

  /**
   * API ìš”ì²­ ë©”íŠ¸ë¦­ ê¸°ë¡
   */
  recordMetric(metric: ApiMetrics): void {
    // ë©”íŠ¸ë¦­ ì €ì¥
    this.metrics.push(metric)

    // ë©”íŠ¸ë¦­ í¬ê¸° ì œí•œ
    if (this.metrics.length > this.maxMetricsSize) {
      this.metrics = this.metrics.slice(-this.maxMetricsSize)
    }

    // ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„ ì—…ë°ì´íŠ¸
    this.updateEndpointStats(metric)

    // ì„±ëŠ¥ ì•Œë¦¼ ì²´í¬
    this.checkPerformanceAlerts(metric)

    // ë””ë²„ê·¸ ë¡œê¹…
    if (metric.responseTime > this.performanceThresholds.slow) {
      logger.warn('ëŠë¦° API ì‘ë‹µ ê°ì§€', {
        component: 'ApiPerformanceMonitor',
        endpoint: metric.endpoint,
        method: metric.method,
        responseTime: metric.responseTime,
        statusCode: metric.statusCode,
        userId: metric.userId,
      })
    }
  }

  /**
   * ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„ ì—…ë°ì´íŠ¸
   */
  private updateEndpointStats(metric: ApiMetrics): void {
    const key = `${metric.method} ${metric.endpoint}`
    const stats = this.endpointStats.get(key) || {
      requestCount: 0,
      totalResponseTime: 0,
      responseTimes: [],
      errorCount: 0,
      lastRequestTime: 0,
    }

    stats.requestCount++
    stats.totalResponseTime += metric.responseTime
    stats.responseTimes.push(metric.responseTime)
    stats.lastRequestTime = metric.timestamp

    if (metric.statusCode >= 400) {
      stats.errorCount++
    }

    // ì‘ë‹µ ì‹œê°„ ë°°ì—´ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œë§Œ ë³´ê´€)
    if (stats.responseTimes.length > 1000) {
      stats.responseTimes = stats.responseTimes.slice(-1000)
    }

    this.endpointStats.set(key, stats)
  }

  /**
   * ì„±ëŠ¥ ì•Œë¦¼ ì²´í¬
   */
  private checkPerformanceAlerts(metric: ApiMetrics): void {
    const { responseTime, endpoint, method, statusCode } = metric

    // ì‹¬ê°í•œ ì‘ë‹µ ì‹œê°„ ì§€ì—°
    if (responseTime > this.performanceThresholds.critical) {
      logger.error('ì‹¬ê°í•œ API ì‘ë‹µ ì§€ì—°', {
        component: 'ApiPerformanceMonitor',
        endpoint,
        method,
        responseTime,
        threshold: this.performanceThresholds.critical,
        alert: 'CRITICAL_RESPONSE_TIME',
      })
    }

    // 5xx ì—ëŸ¬
    if (statusCode >= 500) {
      logger.error('ì„œë²„ ì—ëŸ¬ ë°œìƒ', {
        component: 'ApiPerformanceMonitor',
        endpoint,
        method,
        statusCode,
        responseTime,
        alert: 'SERVER_ERROR',
      })
    }

    // ì—°ì†ëœ ì—ëŸ¬ ì²´í¬
    this.checkConsecutiveErrors(endpoint, method, statusCode >= 400)
  }

  /**
   * ì—°ì†ëœ ì—ëŸ¬ ì²´í¬
   */
  private checkConsecutiveErrors(endpoint: string, method: string, isError: boolean): void {
    const key = `${method} ${endpoint}`
    const recentMetrics = this.metrics
      .filter(m => m.endpoint === endpoint && m.method === method)
      .slice(-5) // ìµœê·¼ 5ê°œ ìš”ì²­

    if (recentMetrics.length >= 5) {
      const errorCount = recentMetrics.filter(m => m.statusCode >= 400).length

      if (errorCount >= 4) {
        logger.error('ì—°ì†ëœ API ì—ëŸ¬ ê°ì§€', {
          component: 'ApiPerformanceMonitor',
          endpoint,
          method,
          errorCount,
          totalChecked: recentMetrics.length,
          alert: 'CONSECUTIVE_ERRORS',
        })
      }
    }
  }

  /**
   * ì „ì²´ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ
   */
  getPerformanceStats(timeRangeMs?: number): PerformanceStats {
    const cutoffTime = timeRangeMs ? Date.now() - timeRangeMs : 0
    const relevantMetrics = this.metrics.filter(m => m.timestamp > cutoffTime)

    if (relevantMetrics.length === 0) {
      return this.getEmptyStats()
    }

    // ì‘ë‹µ ì‹œê°„ ë°°ì—´ (ì •ë ¬ë¨)
    const responseTimes = relevantMetrics
      .map(m => m.responseTime)
      .sort((a, b) => a - b)

    // ê¸°ë³¸ í†µê³„
    const totalRequests = relevantMetrics.length
    const averageResponseTime = relevantMetrics.reduce((sum, m) => sum + m.responseTime, 0) / totalRequests
    const medianResponseTime = this.calculatePercentile(responseTimes, 50)
    const p95ResponseTime = this.calculatePercentile(responseTimes, 95)
    const p99ResponseTime = this.calculatePercentile(responseTimes, 99)

    // ì—ëŸ¬ìœ¨ ê³„ì‚°
    const errorCount = relevantMetrics.filter(m => m.statusCode >= 400).length
    const errorRate = (errorCount / totalRequests) * 100

    // ë¶„ë‹¹ ìš”ì²­ ìˆ˜ ê³„ì‚°
    const timeSpanMs = Math.max(timeRangeMs || (Date.now() - relevantMetrics[0].timestamp), 60000)
    const requestsPerMinute = (totalRequests / timeSpanMs) * 60000

    // ê°€ì¥ ëŠë¦° ì—”ë“œí¬ì¸íŠ¸ë“¤
    const slowestEndpoints = this.getSlowestEndpoints(relevantMetrics)

    // ì—”ë“œí¬ì¸íŠ¸ë³„ ì—ëŸ¬ ìˆ˜
    const errorsByEndpoint = this.getErrorsByEndpoint(relevantMetrics)

    return {
      totalRequests,
      averageResponseTime: Math.round(averageResponseTime),
      medianResponseTime: Math.round(medianResponseTime),
      p95ResponseTime: Math.round(p95ResponseTime),
      p99ResponseTime: Math.round(p99ResponseTime),
      errorRate: Math.round(errorRate * 100) / 100,
      requestsPerMinute: Math.round(requestsPerMinute * 100) / 100,
      slowestEndpoints,
      errorsByEndpoint,
      responseTimePercentiles: [
        this.calculatePercentile(responseTimes, 50),
        this.calculatePercentile(responseTimes, 75),
        this.calculatePercentile(responseTimes, 90),
        this.calculatePercentile(responseTimes, 95),
        this.calculatePercentile(responseTimes, 99),
      ],
    }
  }

  /**
   * ì—”ë“œí¬ì¸íŠ¸ë³„ ìƒì„¸ í†µê³„ ì¡°íšŒ
   */
  getEndpointStats(endpoint: string, method: string): EndpointStats | null {
    const key = `${method} ${endpoint}`
    return this.endpointStats.get(key) || null
  }

  /**
   * ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ í†µê³„ ì¡°íšŒ
   */
  getAllEndpointStats(): Map<string, EndpointStats> {
    return new Map(this.endpointStats)
  }

  /**
   * í¼ì„¼íƒ€ì¼ ê³„ì‚°
   */
  private calculatePercentile(sortedArray: number[], percentile: number): number {
    if (sortedArray.length === 0) return 0

    const index = Math.ceil((percentile / 100) * sortedArray.length) - 1
    return sortedArray[Math.max(0, index)]
  }

  /**
   * ê°€ì¥ ëŠë¦° ì—”ë“œí¬ì¸íŠ¸ë“¤ ì¡°íšŒ
   */
  private getSlowestEndpoints(metrics: ApiMetrics[]): Array<{
    endpoint: string
    averageResponseTime: number
    requestCount: number
  }> {
    const endpointMap = new Map<string, { totalTime: number; count: number }>()

    metrics.forEach(metric => {
      const key = `${metric.method} ${metric.endpoint}`
      const current = endpointMap.get(key) || { totalTime: 0, count: 0 }
      endpointMap.set(key, {
        totalTime: current.totalTime + metric.responseTime,
        count: current.count + 1,
      })
    })

    return Array.from(endpointMap.entries())
      .map(([endpoint, stats]) => ({
        endpoint,
        averageResponseTime: Math.round(stats.totalTime / stats.count),
        requestCount: stats.count,
      }))
      .sort((a, b) => b.averageResponseTime - a.averageResponseTime)
      .slice(0, 10) // ìƒìœ„ 10ê°œ
  }

  /**
   * ì—”ë“œí¬ì¸íŠ¸ë³„ ì—ëŸ¬ ìˆ˜ ì¡°íšŒ
   */
  private getErrorsByEndpoint(metrics: ApiMetrics[]): Record<string, number> {
    const errorMap: Record<string, number> = {}

    metrics
      .filter(m => m.statusCode >= 400)
      .forEach(metric => {
        const key = `${metric.method} ${metric.endpoint}`
        errorMap[key] = (errorMap[key] || 0) + 1
      })

    return errorMap
  }

  /**
   * ë¹ˆ í†µê³„ ê°ì²´ ë°˜í™˜
   */
  private getEmptyStats(): PerformanceStats {
    return {
      totalRequests: 0,
      averageResponseTime: 0,
      medianResponseTime: 0,
      p95ResponseTime: 0,
      p99ResponseTime: 0,
      errorRate: 0,
      requestsPerMinute: 0,
      slowestEndpoints: [],
      errorsByEndpoint: {},
      responseTimePercentiles: [0, 0, 0, 0, 0],
    }
  }

  /**
   * ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
   */
  generatePerformanceReport(timeRangeMs: number = 3600000): string {
    const stats = this.getPerformanceStats(timeRangeMs)
    const timeRangeHours = timeRangeMs / (1000 * 60 * 60)

    return `
=== API ì„±ëŠ¥ ë¦¬í¬íŠ¸ (ìµœê·¼ ${timeRangeHours}ì‹œê°„) ===

ğŸ“Š ì „ì²´ í†µê³„:
- ì´ ìš”ì²­ ìˆ˜: ${stats.totalRequests.toLocaleString()}
- í‰ê·  ì‘ë‹µ ì‹œê°„: ${stats.averageResponseTime}ms
- ì¤‘ê°„ê°’ ì‘ë‹µ ì‹œê°„: ${stats.medianResponseTime}ms
- 95í¼ì„¼íƒ€ì¼: ${stats.p95ResponseTime}ms
- 99í¼ì„¼íƒ€ì¼: ${stats.p99ResponseTime}ms
- ì—ëŸ¬ìœ¨: ${stats.errorRate}%
- ë¶„ë‹¹ ìš”ì²­ ìˆ˜: ${stats.requestsPerMinute}

ğŸŒ ê°€ì¥ ëŠë¦° ì—”ë“œí¬ì¸íŠ¸:
${stats.slowestEndpoints.slice(0, 5).map(ep =>
  `- ${ep.endpoint}: ${ep.averageResponseTime}ms (${ep.requestCount}íšŒ ìš”ì²­)`
).join('\n')}

âŒ ì—ëŸ¬ê°€ ë§ì€ ì—”ë“œí¬ì¸íŠ¸:
${Object.entries(stats.errorsByEndpoint).slice(0, 5).map(([endpoint, count]) =>
  `- ${endpoint}: ${count}íšŒ ì—ëŸ¬`
).join('\n')}

ğŸ“ˆ ì‘ë‹µ ì‹œê°„ ë¶„í¬:
- 50%: ${stats.responseTimePercentiles[0]}ms
- 75%: ${stats.responseTimePercentiles[1]}ms
- 90%: ${stats.responseTimePercentiles[2]}ms
- 95%: ${stats.responseTimePercentiles[3]}ms
- 99%: ${stats.responseTimePercentiles[4]}ms
`
  }

  /**
   * ë©”íŠ¸ë¦­ ë°ì´í„° ì •ë¦¬
   */
  cleanup(olderThanMs: number = 24 * 60 * 60 * 1000): number {
    const cutoffTime = Date.now() - olderThanMs
    const initialLength = this.metrics.length

    this.metrics = this.metrics.filter(m => m.timestamp > cutoffTime)

    const removedCount = initialLength - this.metrics.length

    logger.info('API ë©”íŠ¸ë¦­ ë°ì´í„° ì •ë¦¬', {
      component: 'ApiPerformanceMonitor',
      removedCount,
      remainingCount: this.metrics.length,
      cutoffHours: olderThanMs / (1000 * 60 * 60),
    })

    return removedCount
  }

  /**
   * ì„±ëŠ¥ ìµœì í™” ì œì•ˆ ìƒì„±
   */
  getOptimizationSuggestions(): string[] {
    const stats = this.getPerformanceStats()
    const suggestions: string[] = []

    // ëŠë¦° ì‘ë‹µ ì‹œê°„ ì²´í¬
    if (stats.averageResponseTime > 500) {
      suggestions.push(`í‰ê·  ì‘ë‹µ ì‹œê°„ì´ ${stats.averageResponseTime}msë¡œ ë†’ìŠµë‹ˆë‹¤. ìºì‹±ì´ë‚˜ ì¿¼ë¦¬ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.`)
    }

    // ë†’ì€ ì—ëŸ¬ìœ¨ ì²´í¬
    if (stats.errorRate > 5) {
      suggestions.push(`ì—ëŸ¬ìœ¨ì´ ${stats.errorRate}%ë¡œ ë†’ìŠµë‹ˆë‹¤. ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ì„ ì ê²€í•˜ì„¸ìš”.`)
    }

    // 99í¼ì„¼íƒ€ì¼ì´ ë„ˆë¬´ ë†’ì€ ê²½ìš°
    if (stats.p99ResponseTime > 2000) {
      suggestions.push(`99í¼ì„¼íƒ€ì¼ ì‘ë‹µ ì‹œê°„ì´ ${stats.p99ResponseTime}msì…ë‹ˆë‹¤. ìµœì•…ì˜ ê²½ìš°ë¥¼ ìµœì í™”í•˜ì„¸ìš”.`)
    }

    // ëŠë¦° ì—”ë“œí¬ì¸íŠ¸ê°€ ë§ì€ ê²½ìš°
    const slowEndpoints = stats.slowestEndpoints.filter(ep => ep.averageResponseTime > 1000)
    if (slowEndpoints.length > 3) {
      suggestions.push(`${slowEndpoints.length}ê°œì˜ ì—”ë“œí¬ì¸íŠ¸ê°€ 1ì´ˆ ì´ìƒ ê±¸ë¦½ë‹ˆë‹¤. ê°œë³„ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.`)
    }

    return suggestions
  }
}

// ===========================================
// ì „ì—­ ì„±ëŠ¥ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
// ===========================================

export const apiPerformanceMonitor = new ApiPerformanceMonitor()

// ===========================================
// Express/Next.js ë¯¸ë“¤ì›¨ì–´ í•¨ìˆ˜
// ===========================================

/**
 * API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´
 */
export function createPerformanceMiddleware() {
  return (req: any, res: any, next: any) => {
    const startTime = Date.now()

    // ì‘ë‹µ ì™„ë£Œ ì‹œ ë©”íŠ¸ë¦­ ê¸°ë¡
    res.on('finish', () => {
      const responseTime = Date.now() - startTime

      apiPerformanceMonitor.recordMetric({
        endpoint: req.route?.path || req.url || 'unknown',
        method: req.method,
        responseTime,
        statusCode: res.statusCode,
        userId: req.user?.userId,
        userAgent: req.get('User-Agent'),
        ip: req.ip || req.connection.remoteAddress,
        timestamp: Date.now(),
        error: res.statusCode >= 400 ? `HTTP ${res.statusCode}` : undefined,
      })
    })

    next()
  }
}

/**
 * ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
 */
export function startPerformanceMonitoring(intervalMs: number = 300000): NodeJS.Timeout {
  return setInterval(() => {
    const stats = apiPerformanceMonitor.getPerformanceStats(intervalMs)

    logger.info('API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸', {
      component: 'ApiPerformanceMonitor',
      stats: {
        totalRequests: stats.totalRequests,
        averageResponseTime: stats.averageResponseTime,
        errorRate: stats.errorRate,
        requestsPerMinute: stats.requestsPerMinute,
        p95ResponseTime: stats.p95ResponseTime,
      },
      suggestions: apiPerformanceMonitor.getOptimizationSuggestions(),
    })

    // 24ì‹œê°„ ì´ìƒ ëœ ë©”íŠ¸ë¦­ ì •ë¦¬
    apiPerformanceMonitor.cleanup()
  }, intervalMs)
}